{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "4 Aussmptions: Linearity; Independence of observations; Homoscedasticity (var(y) constant for x); Normality (fixed x, y is normal)\n",
    "\n",
    "Minimize \"Mean Squred Error\" to estimate parameters or called \"Ordinary Least Squares\" \n",
    "\n",
    "OLS has five assumptions: Linearity(in parameters); Random sampling of obs; Zero mean of error|x; No multicollinearity; Homoscedasticity and no autocorrelation among errors\n",
    "\n",
    "Implementation: key logic - initialize parameters; compute parameters; update gradient; helper funtion for details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "def linear_regression(x,\n",
    "                     y,\n",
    "                     iterations = 100,\n",
    "                     learning_rate = 0.005):\n",
    "    n, p = len(x), len(x[0])\n",
    "    beta_0, beta_other = initialize_parameter(p)\n",
    "    for _ in range(iterations):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradient(x,\n",
    "                                                               y,\n",
    "                                                               beta_0,\n",
    "                                                               beta_other,\n",
    "                                                               n,\n",
    "                                                               p)\n",
    "        beta_0, beta_other = update_parameter(beta_0,\n",
    "                                             beta_other,\n",
    "                                             learning_rate,\n",
    "                                             gradient_beta_0,\n",
    "                                             gradient_beta_other)\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameter(dim):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for _ in range(dim)]\n",
    "    return beta_0, beta_other\n",
    "\n",
    "def compute_gradient(x,y,beta_0,beta_other,n,dim):\n",
    "    gradient_beta_0 = 0 \n",
    "    gradient_beta_other = [0] * dim\n",
    "    \n",
    "    for i in range(n):\n",
    "        y_i_hat = sum(x[i][j]*beta_other[j] for j in range(dim)) + beta_0\n",
    "        derror_dy_i = -2*(y[i] - y_i_hat)\n",
    "        gradient_beta_0 += derror_dy_i / n\n",
    "        for j in range(p):\n",
    "            gradient_beta_other[j] += derror_dy * x[i][j] / n\n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "def update_parameter(beta_0, beta_other, gradient_beta_0, gradient_beta_other, learning_rate):\n",
    "    beta_0 += learning_rate * gradient_beta_0\n",
    "    for j in range(len(beta_other)):\n",
    "        beta_other[j] += learning_rate * beta_other[j]\n",
    "    return beta_0, beta_other    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "Binary classification problem (compute probability & classify w/ threshold)\n",
    "MLE -> Loss function = -1/n*ln(MLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x,y,iterations=100,learning_rate=0.05):\n",
    "    n,p = len(x), len(x[0])\n",
    "    beta_0, beta_other = initialize_parameter(p)\n",
    "    \n",
    "    for _ in range(len(iterations)):\n",
    "        gradient_beta_0, gradient_beta_other = compute_gradient(x,\n",
    "                                                                y,\n",
    "                                                                beta_0,\n",
    "                                                                beta_other,\n",
    "                                                                n,\n",
    "                                                                p)\n",
    "        beta_0, beta_other = update_parameters(beta_0,\n",
    "                                              beta_other,\n",
    "                                              gradient_beta_0,\n",
    "                                              gradient_beta_other,\n",
    "                                              learning_rate) # add mini_batch size if using mini batch gradient descent\n",
    "    return beta_0, beta_other     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameter(dim):\n",
    "    beta_0 = 0\n",
    "    beta_other = [random.random() for i in range(dim)]\n",
    "    return beta_0, beta_other\n",
    "\n",
    "def sigmoid(x,beta_0,beta_other):\n",
    "    return 1 / (1+np.exp(-(x.dot(beta_other)+beta_0)))\n",
    "\n",
    "def compute_gradient(x,y,beta_0,beta_other,n,p):\n",
    "    # 1. initialize gradients\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0] * p\n",
    "    \n",
    "    # 2. loop through the samples to add up the gradient at each sample\n",
    "    for i,point in enumerate(x):\n",
    "        pred = sigmoid(point,beta_0,beta_other)\n",
    "        gradient_beta_0 += (y[i]-pred) / n\n",
    "        for j,feature in enumerate(point):\n",
    "            gradient_beta_other[j] += (y[i]-pred) / n * feature\n",
    "    # 3. return the gradients\n",
    "    return gradient_beta_0, gradient_beta_other\n",
    "\n",
    "def update_parameters(beta_0,beta_other,gradient_beta_0,gradient_beta_other,learning_rate):\n",
    "    beta_0 += gradient_beta_0 * learning_rate\n",
    "    for j in range(len(beta_other)):\n",
    "        beta_other[j] += gradient_beta_other[j] * learning_rate\n",
    "    return beta_0, beta_other \n",
    "\n",
    "def compute_gradient_mini_batch(x,y,beta_0,beta_other,n,p,batch_size):\n",
    "    gradient_beta_0 = 0\n",
    "    gradient_beta_other = [0] * p\n",
    "    \n",
    "    for _ in range(batch_size):\n",
    "        i = random.randint(0,n-1)\n",
    "        point = x[i]\n",
    "        pred = sigmoid(point,beta_0,beta_other)\n",
    "        gradient_beta_0 += (pred-y[i]) / n\n",
    "        for j,feature in enumerate(beta_0):\n",
    "            gradient_beta_other[j] += (pred-y[i]) / n * point[i][j]\n",
    "    return beta_0, beta_other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors\n",
    "Non-parameteric method; one parameter k and distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def train(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    def distanct(self, x1, x2):\n",
    "        # use Euclidean distance: square root of MSE\n",
    "        SSE = sum([(x1_i-x2_i)**2 for x1_i, x2_i in zip(x1,x2)])\n",
    "        return sqrt(SSE)\n",
    "    def predict_continuous(self, new_x, k): # regression\n",
    "        distance_label = [(self.distance(new_x, train_x), train_y) for train_x, train_y in zip(self.x, self.y)]\n",
    "        k_neighbors = sorted(distance_label)[:k]\n",
    "        return sum(label for _, label in k_neighbors)/k\n",
    "    def predict_categorical(self, new_x, k): # classification\n",
    "        distance_label = [(self.distance(new_x, train_x), train_y) for train_x, train_y in zip(self.x, self.y)]\n",
    "        k_neighbors = sorted(distance_label)[:k]\n",
    "        k_neighbors_labels = [label for _, label in k_neighbors]\n",
    "        return Counter(k_neighbors_labels).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Means Clustering\n",
    "* Step 1: initialize k entroids\n",
    "* Step 2: calculate distance, pick class and update centroids\n",
    "* Step 3: check the threshold, if smaller than threshold stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for simplicity use two dimensional data point as example\n",
    "def K_Means(data,k):\n",
    "    centroids = centroids_initialize(data,k)\n",
    "    while True: # until converge\n",
    "        old_centroids = centroids\n",
    "        labels = get_labels(data, centroids, k)\n",
    "        centroids = update_centroids(data, labels)\n",
    "        \n",
    "        if should_stop(old_centroids, centroids):\n",
    "            break\n",
    "    return labels # need to return the clustering classes instead of centroids!\n",
    "\n",
    "def random_sample(low, high):\n",
    "    return low + (high - low)*random.random() # 0~1\n",
    "\n",
    "def centroids_initialize(data,k):\n",
    "    x_min, x_max = float('inf'), float('-inf')\n",
    "    y_min, y_max = float('inf'), float('-inf')\n",
    "    \n",
    "    for point in data:\n",
    "        x_min = min(x_min, point[0])\n",
    "        x_max = max(x_max, point[0])\n",
    "        y_min = min(y_min, point[1])\n",
    "        y_max = max(y_max, point[1])\n",
    "    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        centroids.append([random_sample(x_min, x_max),\n",
    "                         random_sample(y_min, y_max)])\n",
    "    return centroids\n",
    "\n",
    "def distance(point1, point2):\n",
    "    return ( (point1[0] - point2[0])**2+(point1[1] - point2[1])**2 ) **0.5\n",
    "\n",
    "def get_labels(data, centroids):\n",
    "    \n",
    "    labels = []\n",
    "    for x in data:\n",
    "        label = -1\n",
    "        min_distance = float('inf')        \n",
    "        for j,centroid in enumerate(centroids):\n",
    "            distance_j = distance(x, centroid)\n",
    "            if distance_j < min_distance:\n",
    "                min_distance = distance_j\n",
    "                label = j\n",
    "        labels.append(label)\n",
    "    return labels\n",
    "\n",
    "def update_centroids(data,labels,k):\n",
    "    new_centroids = [[0,0] * k] # get nuerator\n",
    "    counts = [] # get denominator\n",
    "    for x,label in zip(data,labels):\n",
    "        new_centroids[label][0] += x[0]\n",
    "        new_centroids[label][1] += x[1]\n",
    "        count[label] += 1\n",
    "    for j,(x,y) in enumerate(new_centroids):\n",
    "        new_centroids[j] = [x/count[j], y/count[j]]\n",
    "    return new_centroids\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "* Step 1: Create root -> requires find the best split by traversing all the attributes and all the values, which then require test_split function for each attribute at a value and gini calculation function.\n",
    "* Step 2: Recursively split for childrens -> DFS pattern; use max_depth, min_child to early return; create terminal nodes\n",
    "* Step 3: Return the tree\n",
    "* https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose the last column is target\n",
    "\n",
    "def compute_gini(groups, classes):\n",
    "    gini = 0.0\n",
    "    n_instances = sum([len(group) for group in groups])\n",
    "    # gini of each split = sum of (gini of each group * size / total sample size)\n",
    "    # gini of each group = 1 - sum of (p^2 of each class)\n",
    "    \n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size = 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        for class_val in classes:\n",
    "            p = [row[-1] for row in group].count(class_val) / size\n",
    "            score += p**2\n",
    "        gini += (1 - score) * (size / n_instances)\n",
    "    return gini\n",
    "         \n",
    "def test_split(rows, index, index_value):\n",
    "    left, right = list(), list()\n",
    "    for row in rows:\n",
    "        if row[index] > index_value:\n",
    "            right.append(row)\n",
    "        else:\n",
    "            left.append(row)\n",
    "    return left, right\n",
    "\n",
    "def get_split(rows):\n",
    "    # best_index, best_index_value, best_groups, best_gini\n",
    "    classes = list(set(row[-1] for row in rows))\n",
    "    best_index, best_index_value, best_groups, best_gini = 999, 999, [], 999\n",
    "    for index in range(len(rows[0])-1): # not including the target\n",
    "        for row in rows:\n",
    "            groups = test_split(rows, index, row[index])\n",
    "            gini = compute_gini(groups, classes)\n",
    "            if gini < best_gini:\n",
    "                best_index, best_index_value, best_groups, best_gini = index, row[index], groups, gini\n",
    "    return {'index': best_index,\n",
    "           'value': best_index_value,\n",
    "           'groups': best_groups}\n",
    "\n",
    "def to_terminal(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key = outcomes.count) # the mode class\n",
    "\n",
    "def split(node, max_depth, min_child, depth):\n",
    "    left, right = node['groups']\n",
    "    del node['groups']\n",
    "    # below are certain situations for terminal node, o.w. recursive\n",
    "    if not left or not right: # actually no split\n",
    "        node['left'] = node['right'] = to_terminal(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n",
    "    if len(left) <=  min_child:\n",
    "        node['left'] = to_terminal(left)\n",
    "    else:\n",
    "        node['left'] = get_split(left)\n",
    "        split(node['left'], max_depth, min_child, depth + 1)\n",
    "    if len(right) <=  min_child:\n",
    "        node['right'] = to_terminal(right)\n",
    "    else:\n",
    "        node['right'] = get_split(right)\n",
    "        split(node['right'], max_depth, min_child, depth + 1)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(train, max_depth, min_child):\n",
    "    root = get_split(train)\n",
    "    split(root, max_depth, min_child, 1)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(): # did not understand well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(node, row):\n",
    "    \n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cracking the machine learning questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

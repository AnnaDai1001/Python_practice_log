{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN Model from Scratch with Numpy\n",
    "Recurrent Neural Networks (RNNs) are a kind of neural network that specialize in processing sequences. They’re often used in Natural Language Processing (NLP) tasks because of their effectiveness in handling text.\n",
    "\n",
    "Why? One issue with vanilla neural nets (and also CNNs) is that they only work with pre-determined sizes: they take fixed-size inputs and produce fixed-size outputs. RNNs are useful because they let us have variable-length sequences as both inputs and outputs (one, many) to  (one, many)\n",
    "\n",
    "e.g. many to many - machine translation; many to one: sentiment analysis\n",
    "\n",
    "Here’s what makes a RNN recurrent: it uses the same weights for each step. More specifically, a typical vanilla RNN uses only 3 sets of weights to perform its calculations: W_xh, W_hh, W_hy; bias: b_h, b_y\n",
    "\n",
    "h_t = tanh(W_xh * x_t + W_hh * h_t-1 + b_h); y = W_hy * h_t + b_y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the blog here: https://victorzhou.com/tag/neural-networks/page/2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://victorzhou.com/blog/intro-to-rnns/\n",
    "# suppose we already load the train and test data\n",
    "from data import train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 unique words in the text\n"
     ]
    }
   ],
   "source": [
    "# create the vocabulary\n",
    "vocab = list(set(w for text in train_data.keys() for w in text.split(' ')))\n",
    "vocab_size = len(vocab)\n",
    "print('%d unique words in the text' %vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "and\n"
     ]
    }
   ],
   "source": [
    "# attach indices to words\n",
    "word_idx = {w:i for i,w in enumerate(vocab)}\n",
    "idx_word = {i:w for i,w in enumerate(vocab)}\n",
    "print(word_idx['good'])\n",
    "print(idx_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# return an array containing one hot encoding vector of the words\n",
    "def createInputs(text):\n",
    "    inputs = []\n",
    "    for w in text.split(' '): # one sentence\n",
    "        encode = np.zeros((vocab_size, 1))\n",
    "        encode[word_idx[w]] = 1\n",
    "        inputs.append(encode)\n",
    "    return inputs        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights of artificial neural networks must be initialized to small random numbers. This is because this is an expectation of the stochastic optimization algorithm used to train the model, called stochastic gradient descent.\n",
    "\n",
    "It is possible and common to initialize the biases to be zero, since the asymmetry breaking is provided by the small random numbers in the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you see an @ in the middle of a line, that's a different thing, matrix multiplication.\n",
    "# https://stackoverflow.com/questions/6392739/what-does-the-at-symbol-do-in-python\n",
    "from numpy.random import randn\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, input_size, output_size, hidden_size = 64):\n",
    "        \n",
    "        self.Wxh = randn(hidden_size, input_size) / 1000 # explained above\n",
    "        self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "        self.Why = randn(output_size, hidden_size) / 1000\n",
    "        \n",
    "        self.bh = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((output_size, 1)) \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        h = np.zeros((self.Whh.shape[0], 1))\n",
    "        \n",
    "        self.last_inputs = inputs\n",
    "        self.last_hs = {0: h}\n",
    "        \n",
    "        for i,x in enumerate(inputs):\n",
    "            h = np.tanh(self.Wxh@x + self.Whh@h + self.bh)\n",
    "            self.last_hs[i+1] = h\n",
    "        \n",
    "        y = self.Why@h + self.by\n",
    "    \n",
    "        return y, h\n",
    "    \n",
    "    def backprop(self, d_y, learn_rate = 2e-2):\n",
    "        \n",
    "        n = len(self.last_inputs)\n",
    "        \n",
    "        d_Why = d_y @ self.last_hs[n].T\n",
    "        d_by = d_y\n",
    "        \n",
    "            # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "        d_Whh = np.zeros(self.Whh.shape)\n",
    "        d_Wxh = np.zeros(self.Wxh.shape)\n",
    "        d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "        # Calculate dL/dh for the last h.\n",
    "        d_h = self.Why.T @ d_y\n",
    "\n",
    "        # Backpropagate through time.\n",
    "        for t in reversed(range(n)):\n",
    "          # An intermediate value: dL/dh * (1 - h^2)\n",
    "          temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "\n",
    "          # dL/db = dL/dh * (1 - h^2)\n",
    "          d_bh += temp\n",
    "\n",
    "          # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "          d_Whh += temp @ self.last_hs[t].T\n",
    "\n",
    "          # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "          d_Wxh += temp @ self.last_inputs[t].T\n",
    "\n",
    "          # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "          d_h = self.Whh @ temp\n",
    "\n",
    "        # Clip to prevent exploding gradients.\n",
    "        for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "            np.clip(d, -1, 1, out=d)\n",
    "\n",
    "        # Update weights and biases using gradient descent.\n",
    "        self.Whh -= learn_rate * d_Whh\n",
    "        self.Wxh -= learn_rate * d_Wxh\n",
    "        self.Why -= learn_rate * d_Why\n",
    "        self.bh -= learn_rate * d_bh\n",
    "        self.by -= learn_rate * d_by\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(ex):\n",
    "    return np.exp(ex)/sum(np.exp(ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49999712]\n",
      " [0.50000288]]\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "inputs = createInputs('i am very good')\n",
    "out, h = rnn.forward(inputs)\n",
    "probs = softmax(out)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def processData(data, backprop=True):\n",
    "    '''\n",
    "      Returns the RNN's loss and accuracy for the given data.\n",
    "      - data is a dictionary mapping text to True or False.\n",
    "      - backprop determines if the backward phase should be run.\n",
    "      '''\n",
    "    items = list(data.items())\n",
    "    random.shuffle(items)\n",
    "\n",
    "    loss = 0\n",
    "    num_correct = 0\n",
    "\n",
    "    for x, y in items:\n",
    "        inputs = createInputs(x)\n",
    "        target = int(y)\n",
    "\n",
    "        # Forward\n",
    "        out, _ = rnn.forward(inputs)\n",
    "        probs = softmax(out)\n",
    "\n",
    "        # Calculate loss / accuracy\n",
    "        loss -= np.log(probs[target])\n",
    "        num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "        if backprop:\n",
    "          # Build dL/dy\n",
    "            d_L_d_y = probs\n",
    "            d_L_d_y[target] -= 1\n",
    "\n",
    "          # Backward\n",
    "            rnn.backprop(d_L_d_y)\n",
    "\n",
    "    return loss / len(data), num_correct / len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 100\n",
      "Train:\tLoss 0.689 | Accuracy: 0.552\n",
      "Test:\tLoss 0.698 | Accuracy: 0.500\n",
      "--- Epoch 200\n",
      "Train:\tLoss 0.672 | Accuracy: 0.586\n",
      "Test:\tLoss 0.722 | Accuracy: 0.450\n",
      "--- Epoch 300\n",
      "Train:\tLoss 0.144 | Accuracy: 0.983\n",
      "Test:\tLoss 0.222 | Accuracy: 0.900\n",
      "--- Epoch 400\n",
      "Train:\tLoss 0.040 | Accuracy: 1.000\n",
      "Test:\tLoss 0.079 | Accuracy: 0.950\n",
      "--- Epoch 500\n",
      "Train:\tLoss 0.010 | Accuracy: 1.000\n",
      "Test:\tLoss 0.028 | Accuracy: 1.000\n",
      "--- Epoch 600\n",
      "Train:\tLoss 0.005 | Accuracy: 1.000\n",
      "Test:\tLoss 0.020 | Accuracy: 1.000\n",
      "--- Epoch 700\n",
      "Train:\tLoss 0.003 | Accuracy: 1.000\n",
      "Test:\tLoss 0.022 | Accuracy: 1.000\n",
      "--- Epoch 800\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.021 | Accuracy: 1.000\n",
      "--- Epoch 900\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.019 | Accuracy: 1.000\n",
      "--- Epoch 1000\n",
      "Train:\tLoss 0.002 | Accuracy: 1.000\n",
      "Test:\tLoss 0.015 | Accuracy: 1.000\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "    train_loss, train_acc = processData(train_data)\n",
    "\n",
    "    if epoch % 100 == 99:\n",
    "        print('--- Epoch %d' % (epoch + 1))\n",
    "        print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "        test_loss, test_acc = processData(test_data, backprop=False)\n",
    "        print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
